/*
 * ------------------------------------------------------------------------
 *
 *  Copyright by KNIME AG, Zurich, Switzerland
 *  Website: http://www.knime.com; Email: contact@knime.com
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License, Version 3, as
 *  published by the Free Software Foundation.
 *
 *  This program is distributed in the hope that it will be useful, but
 *  WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, see <http://www.gnu.org/licenses>.
 *
 *  Additional permission under GNU GPL version 3 section 7:
 *
 *  KNIME interoperates with ECLIPSE solely via ECLIPSE's plug-in APIs.
 *  Hence, KNIME and ECLIPSE are both independent programs and are not
 *  derived from each other. Should, however, the interpretation of the
 *  GNU GPL Version 3 ("License") under any applicable laws result in
 *  KNIME and ECLIPSE being a combined program, KNIME AG herewith grants
 *  you the additional permission to use and propagate KNIME together with
 *  ECLIPSE with only the license terms in place for ECLIPSE applying to
 *  ECLIPSE and the GNU GPL Version 3 applying for KNIME, provided the
 *  license terms of ECLIPSE themselves allow for the respective use and
 *  propagation of ECLIPSE together with KNIME.
 *
 *  Additional permission relating to nodes for KNIME that extend the Node
 *  Extension (and in particular that are based on subclasses of NodeModel,
 *  NodeDialog, and NodeView) and that only interoperate with KNIME through
 *  standard APIs ("Nodes"):
 *  Nodes are deemed to be separate and independent programs and to not be
 *  covered works.  Notwithstanding anything to the contrary in the
 *  License, the License does not apply to Nodes, you are not required to
 *  license Nodes under the License, and you are granted a license to
 *  prepare and propagate Nodes, in each case even if such Nodes are
 *  propagated with or for interoperation with KNIME.  The owner of a Node
 *  may freely choose the license terms applicable to such Node, including
 *  when such Node is propagated with or for interoperation with KNIME.
 * ---------------------------------------------------------------------
 *
 * History
 *   Jun 21, 2020 (carlwitt): created
 */
package org.knime.core.data.join;

import java.util.Optional;
import java.util.function.ObjLongConsumer;
import java.util.function.Supplier;

import org.knime.core.data.DataCell;
import org.knime.core.data.DataRow;
import org.knime.core.data.container.CloseableRowIterator;
import org.knime.core.data.join.HybridHashJoin.DiskBackedHashPartitions.DiskBucket;
import org.knime.core.data.join.JoinImplementation.JoinProgressMonitor;
import org.knime.core.data.join.JoinSpecification.InputTable;
import org.knime.core.data.join.results.JoinContainer;
import org.knime.core.data.join.results.JoinResults;
import org.knime.core.node.BufferedDataTable;
import org.knime.core.node.CanceledExecutionException;
import org.knime.core.node.CanceledExecutionException.CancelChecker;
import org.knime.core.node.ExecutionContext;

/**
 * Implements a nested loop join that can have extremely small memory footprint, at the cost of additional iterations
 * over the probe input. Used if the {@link HybridHashJoin} has to flush rows to disk because of scarce heap memory. In
 * this case, input tables are split into {@link DiskBucket}s which are joined using {@link BlockHashJoin}. Ideally, the
 * one of the table partitions is small enough such that {@link BlockHashJoin} can index it in memory and needs only a
 * single pass over the other partition.
 *
 * <h1>Usage</h1>
 *
 * {@link DiskBucket}s store only the columns of the input tables that are necessary for joining (working table format).
 * Thus, joining the disk buckets is a little bit different from joining the original input tables. The new
 * {@link JoinSpecification} is generated by deriving an intermediate {@link JoinSpecification} by reusing the join and
 * include column names and swapping out the original input for the working tables, see
 * {@link JoinSpecification#specWith(JoinTableSettings, JoinTableSettings)}.
 *
 * <h1>Internals</h1>
 *
 * The smaller table is considered the hash input, the other table becomes the probe input. An outer loop performs a
 * single pass over the rows of the hash input. The rows are indexed in a {@link HashIndex} until memory is running low.
 * Then, a complete pass over the probe input is performed and the {@link HashIndex} is discarded. The outer loop then
 * continues to index hash input rows and does more passes over the probe input every time memory is running low. <br/>
 * <br/>
 *
 * Note that when joining two {@link DiskBucket}s P and H originally from the probe table and hash table, P can become
 * the hash input if fewer rows from the original probe input have been mapped to P than rows from the original hash
 * input to H.
 *
 * @author Carl Witt, KNIME AG, Zurich, Switzerland
 */
class BlockHashJoin {

    private final ExecutionContext m_exec;

    //    private final
    private final JoinProgressMonitor m_progress;

    final JoinSpecification m_joinSpecification;

    /**
     * @param exec
     * @param progress
     * @param joinSpecification
     * @param extractRowOffsets
     */
    BlockHashJoin(final ExecutionContext exec, final JoinProgressMonitor progress,
        final JoinSpecification joinSpecification, final boolean extractRowOffsets) {
        m_exec = exec;
        m_progress = progress;
        m_joinSpecification = joinSpecification;
        m_extractRowOffsets = extractRowOffsets;
    }

    final boolean m_extractRowOffsets;

    /**
     * @param results where to put join results (matches and unmatched rows)
     * @param leftUnmatchedRows unmatched row handler for unmatched rows from the left table
     * @param rightUnmatchedRows unmatched row handler for unmatched rows from the right table
     * @throws CanceledExecutionException
     */
    void join(final JoinContainer results) throws CanceledExecutionException {

        // if only one of the input tables is present, add its rows to the unmatched results
        if (incompleteInput(m_joinSpecification, results)) {
            return;
        }

        InputTable hashSide = HashIndex.smallerTable(m_joinSpecification);
        InputTable probeSide = hashSide.other();

//        System.out.println(String.format("hashSide=%s", hashSide));

        JoinTableSettings hashSettings = m_joinSpecification.getSettings(hashSide);
        JoinTableSettings probeSettings = m_joinSpecification.getSettings(probeSide);

        // if either bucket is empty, we're done joining those buckets.
        Optional<BufferedDataTable> probeInput = probeSettings.getTable();
        Optional<BufferedDataTable> hashInput = hashSettings.getTable();

        ObjLongConsumer<DataRow> unmatchedHashRows = extractOffsets(results.unmatched(hashSettings.getSide()));

        BufferedDataTable probe = probeInput.orElseThrow(IllegalStateException::new);
        BufferedDataTable hash = hashInput.orElseThrow(IllegalStateException::new);

//        System.out.println(String.format("extractRowOffsets=%s", m_extractRowOffsets));

        // this is an incomplete index, as it represents only the hash rows indexed in one pass over the probe input
        Supplier<HashIndex> newHashIndex =
            () -> new HashIndex(m_joinSpecification, results, hashSide, m_progress::isCanceled);
        HashIndex index = newHashIndex.get();

        try (CloseableRowIterator hashRows = hash.iterator()) {

            long rowOffset = 0;
            while (hashRows.hasNext()) {

                DataRow hashRow = hashRows.next();
                if (m_extractRowOffsets) {
                    rowOffset = OrderedRow.OFFSET_EXTRACTOR.applyAsLong(hashRow);
                }

                DataCell[] joinAttributeValues = JoinTuple.get(hashSettings, hashRow);

                if (joinAttributeValues == null) {
//                    System.out.println(String.format("unmatched hash Row=%s", hashRow));
                    unmatchedHashRows.accept(hashRow, rowOffset);
                } else {
//                    System.out.println(
//                        String.format("index hash row %s using %s", hashRow, Arrays.toString(joinAttributeValues)));
                    index.addHashRow(joinAttributeValues, hashRow, rowOffset);
                }

                // if memory is running low, do a pass over the probe input to be able to clear the hash index
                boolean memoryLow = m_progress.isMemoryLow(100);
                if (memoryLow) {
                    // since we're doing several passes over the probe side, enable deferred handling of unmatched probe rows
                    results.setDeferUnmatchedRows(probeSide, true);

                    // switch from caching unmatched rows to just marking unmatched rows and collecting them later
                    // TODO pass on the signal via join container probeRowHandler.lowMemory();
                    System.out
                        .println(String.format("Flushing pass over probe input (%s) against partialIndex with %s rows",
                            probeSide, index.numAddedRows()));
                    singlePass(probe, index, unmatchedHashRows);
                    index = newHashIndex.get();
                }

                m_progress.setProgressAndCheckCanceled(0.333 * rowOffset / hash.size());

                rowOffset++;

            } // all hash input rows indexed

        } // close hash input row iterator

        // process pending hash index contents
//        System.out.println(String.format("Final pass over probe input (%s) against partialIndex with %s rows",
//            probeSide, index.numAddedRows()));
        singlePass(probe, index, unmatchedHashRows);

    }

    private void singlePass(final BufferedDataTable probe, final HashIndex partialIndex, final ObjLongConsumer<DataRow> unmatchedHashRows)
        throws CanceledExecutionException {

        CancelChecker checkCanceled = CancelChecker.checkCanceledPeriodically(m_exec);
        JoinResults.enumerateWithResources(probe, extractOffsets((t, value) -> {
            try {
                partialIndex.joinSingleRow(t, value);
            } catch (CanceledExecutionException ex) {
                // TODO this is a bit rough
                throw new IllegalStateException();
            }
        }), checkCanceled);

        partialIndex.forUnmatchedHashRows(unmatchedHashRows);

        // all results have been produced in probe-hash row order.
        partialIndex.m_joinContainer.sortedChunkEnd();
    }

    /**
     * Check that both input tables are present. If only one table is present, output the rows of the other table as
     * unmatched rows.
     *
     * @param joinSpecification contains the two input tables
     * @param container where to put unmatched rows
     * @param m_extractRowOffsets whether to
     * @return true if the input is incomplete
     * @throws CanceledExecutionException
     */
    private boolean incompleteInput(final JoinSpecification joinSpecification, final JoinResults container)
        throws CanceledExecutionException {

        if (!joinSpecification.getSettings(InputTable.LEFT).hasTable()
            && !joinSpecification.getSettings(InputTable.RIGHT).hasTable()) {
            return true;
        }

        for (InputTable presentSide : InputTable.values()) {
            JoinTableSettings present = joinSpecification.getSettings(presentSide);
            JoinTableSettings absent = joinSpecification.getSettings(presentSide.other());
            Optional<BufferedDataTable> presentTable = present.getTable();
            if (presentTable.isPresent() && !absent.getTable().isPresent()) {
                // collect rows from present table as unmatched
                if (present.isRetainUnmatched()) {
                    JoinResults.enumerateWithResources(presentTable.get(),
                        extractOffsets(container.unmatched(presentSide)),
                        CancelChecker.checkCanceledPeriodically(m_exec));
                }
                container.sortedChunkEnd();
                // only one table is present.
//                System.out.println("Incomplete input to nested loop join.");
                return true;
            }
        }
        return false;
    }

    final ObjLongConsumer<DataRow> extractOffsets(final ObjLongConsumer<DataRow> rowHandler) {
        if (m_extractRowOffsets) {
            return OrderedRow.extractOffsets(rowHandler);
        } else {
            return rowHandler;
        }
    }
}
